# This config file is used to set simulation parameters. There are 5 configuration sections:
# - Environment
# - TWTL constraint
# - static rewards
# - Augmented MDP rewards
# - Q-learning configuration
# Each section and configuration options are explained in this file


# This option sets the MDP type. The options are "flag-MDP", "tau-MDP", and "static rewards".
# flag-MDP and tau-MDP use rewards calculated from the satisfaction of an STL specification defined later in this file,
# and static rewards uses rewards assigned per state also defined later in this file.
MDP type: static rewards
# MDP type: flag-MDP
# MDP type: tau-MDP

# The environment section defines properties of the grid environment, agent actions, and the MDP representation
environment:

  # Height and width of the grid environment
  height: 6
  width: 6

  # Initial state for the first episode of learning
  init state: [0,0,0]

  # The action uncertainty is the epsilon that determines the probability of epsilon-stochastic transitions. 
  # The overestimated action uncertainty is used in pruning actions to enforce the constraint.
  # The over estimated action uncertainty should be greater than the real action uncertainty.
  real action uncertainty: 0.03 #don't change
  over estimated action uncertainty: 0.08 # e_est

  # Obstacles states defined in the following list will be removed from the MDP such that they can't be visited.
  # obstacles: []
  obstacles: [[0,4,0],[0,5,0],[1,1,0],[2,0,0],[2,1,0],[2,3,0],[3,0,0],[3,1,0],[3,3,0],[4,0,0],[4,5,0],[5,4,0],[5,5,0]]
 
  one way: 
    r20: r14 # r20 -> r14; r14 not -> r20


# The TWTL constraint section defines options for the probabilistically enforced TWTL constraint
TWTL constraint:

  # The following defines the TWTL task that is probabilistically enforced. You must define named regions under "regions".
  # Alternatively, for a simple pickup and delivery tasks, the following can be set to "None", 
  # and "pickup" and "delivery" states must be defined under "regions"
  # TWTL task: '[H^1 pickup]^[0,8]'
  TWTL task: '[H^1 pickup]^[0,8] * [H^1 delivery1]^[0,6] * ([H^1 delivery2]^[0,6] | [H^1 delivery3]^[0,6]) * [H^1 base]^[0,12]'
  # TWTL task: '[H^1 pickup]^[0,6] * [H^1 delivery1]^[0,5] * [H^1 delivery2]^[0,5] * [H^1 base]^[0,12]'
  
  # critical_time: [35]
  critical_time: [8,15,22,35]
  # '[H^1 r2]^[0,4] * ([H^1 r0]^[0,4] | [H^1 r7]^[0,4]) * [H^0 r2]^[0,4]'
  # '[H^1 pickup]^[0,20] * ([H^1 delivery1]^[0,20] | [H^1 delivery2]^[0,20]) * [H^1 base]^[0,20]'
  # '[H^1 r3]^[0,4] * [H^0 r1]^[1,4]'
  # '[H^1 r3]^[0,2] & [H^1 r1]^[0,3]'   sss
  # '[H^1 r3]^[0,2] * [H^0 r1]^[1,2]'
  # '[H^1 pickup]^[0,20] * over estimated action uncertainty([H^1 delivery1]^[0,20] | [H^1 delivery2]^[0,20]) * [H^1 base]^[0,20]'

  # You can define region coordinates that will replace the respective name in the TWTL task.
  # You will have issues if one name is a subset of another (i.e. 'base' and 'base2')
  regions:
    pickup: [2,2,0]
    delivery1: [3,5,0]
    delivery2: [5,2,0]
    delivery3: [0,3,0]
    base: [0,0,0]
    reward: [5,0,0]


  # DFA kind can be "infinity" or "normal". Normal 0.is the full automaton and infinity is the relaxed version..
  DFA kind: normal
  # infinity
  DFA modification type: total
  # 

  # The probability to which the TWTL task will be enforced
  desired satisfaction probability: 0.9 #pr_des----

  # The time horizon used when the custom task is set to "None" for a simple pick up and delivery.
  # This is not used if a TWTL task is given
  time horizon: 62

  # Whether to save a png of the dfa in the output folder
  save dfa: True


# STL objective used in flag-MDP and tau-MDP MDP options
aug-MDP rewards:
  STL expression: G[0,10]F[0,3](((x>1)&(x<2))&((y>3)&(y<4)))


# The region to reward map used in the static rewards MDP option
static rewards:
  reward dict: 
    r6: 1
    r30: 10


# The Q-learning configuration section defines parameters for learning the optimal policy according to either STL based rewards for
# tau-MDP and flag-MDP options or region rewards for the static rewards option.
Q-learning config:

  # The number of episodes for learning
  number of episodes: 5000

  # Parameters used in the Q-value update
  learning rate: 0.1
  discount: 0.99

  # The exploration probability in epison-greedy exploration is decayed as learning proceeds.
  # Exploration probability decay is calculated by solving the following equation: (start * decay^(num_eps - 1) = end)
  explore probability start: 0.6
  explore probability end: 0.05

test_policy config:
  use saved policy: False
  policy_file: 'data/learned_policy.json'
  policy_pic_file: 'data/learned_pic_policy.json'
  number of episodes: 10000
